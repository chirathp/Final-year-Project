{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "YxkrpOitk6bl",
        "outputId": "926cf736-1669-4dda-920d-eaf0853eba73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your music dataset (CSV file) for training:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b76f3eda-02a2-4e3c-bdb0-91d9ada80248\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b76f3eda-02a2-4e3c-bdb0-91d9ada80248\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving 278k_labelled_uri1.csv to 278k_labelled_uri1.csv\n",
            "\n",
            "Dataset loaded successfully!\n",
            "Found 4000 songs\n",
            "\n",
            "Converting numeric labels to strings...\n",
            "\n",
            "Dataset validation complete. Proceeding with training...\n",
            "\n",
            "Training completed successfully!\n",
            "Saved files:\n",
            "- scaler.joblib: Feature scaler\n",
            "- knn_model.joblib: Trained KNN model\n",
            "- processed_data.csv: Processed dataset\n"
          ]
        }
      ],
      "source": [
        "# train.py - Training and model saving component\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "def train_model():\n",
        "    # Upload your dataset\n",
        "    print(\"Please upload your music dataset (CSV file) for training:\")\n",
        "    uploaded = files.upload()\n",
        "    filename = next(iter(uploaded)) if uploaded else None\n",
        "\n",
        "    try:\n",
        "        if not filename:\n",
        "            raise ValueError(\"No file uploaded\")\n",
        "\n",
        "        df = pd.read_csv(filename)\n",
        "        print(\"\\nDataset loaded successfully!\")\n",
        "        print(f\"Found {len(df)} songs\")\n",
        "\n",
        "        # Validate required columns\n",
        "        required_cols = {'labels', 'uri', 'danceability', 'energy', 'valence', 'tempo'}\n",
        "        missing_cols = required_cols - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Convert labels to string if they're numeric\n",
        "        if not pd.api.types.is_string_dtype(df['labels']):\n",
        "            print(\"\\nConverting numeric labels to strings...\")\n",
        "            df['labels'] = df['labels'].astype(str)\n",
        "\n",
        "        print(\"\\nDataset validation complete. Proceeding with training...\")\n",
        "\n",
        "        # Prepare KNN recommender\n",
        "        features = [\"danceability\", \"energy\", \"valence\", \"tempo\"]\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(df[features])\n",
        "\n",
        "        # Set dynamic n_neighbors based on dataset size\n",
        "        n_neighbors = min(5, len(df))\n",
        "        knn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine')\n",
        "        knn.fit(X_scaled)\n",
        "\n",
        "        # Save the trained models and data\n",
        "        joblib.dump(scaler, 'scaler.joblib')\n",
        "        joblib.dump(knn, 'knn_model.joblib')\n",
        "        df.to_csv('processed_data.csv', index=False)\n",
        "\n",
        "        print(\"\\nTraining completed successfully!\")\n",
        "        print(\"Saved files:\")\n",
        "        print(\"- scaler.joblib: Feature scaler\")\n",
        "        print(\"- knn_model.joblib: Trained KNN model\")\n",
        "        print(\"- processed_data.csv: Processed dataset\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during training: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3c8sklulW-E"
      },
      "outputs": [],
      "source": [
        "# # predict.py - Prediction and recommendation component\n",
        "# import pandas as pd\n",
        "# import joblib\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "# from flask import Flask, jsonify\n",
        "# import os\n",
        "\n",
        "# # Initialize Flask app\n",
        "# app = Flask(__name__)\n",
        "# run_with_ngrok(app)\n",
        "\n",
        "# # Load trained models and data\n",
        "# def load_models():\n",
        "#     try:\n",
        "#         scaler = joblib.load('scaler.joblib')\n",
        "#         knn = joblib.load('knn_model.joblib')\n",
        "#         df = pd.read_csv('processed_data.csv')\n",
        "#         return scaler, knn, df\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading models: {str(e)}\")\n",
        "#         return None, None, None\n",
        "\n",
        "# scaler, knn, df = load_models()\n",
        "# features = [\"danceability\", \"energy\", \"valence\", \"tempo\"]\n",
        "\n",
        "# def get_recommendations(emotion):\n",
        "#     \"\"\"Get recommendations using the trained model\"\"\"\n",
        "#     if scaler is None or knn is None or df is None:\n",
        "#         return {\"error\": \"Models not loaded properly\"}\n",
        "\n",
        "#     emotion_str = str(emotion).lower()\n",
        "#     emotion_songs = df[df['labels'].str.lower() == emotion_str]\n",
        "\n",
        "#     if len(emotion_songs) == 0:\n",
        "#         return {\"error\": f\"No songs found for emotion: {emotion}\"}\n",
        "\n",
        "#     # KNN-based recommendations\n",
        "#     avg_features = emotion_songs[features].mean().values.reshape(1, -1)\n",
        "#     avg_features_scaled = scaler.transform(avg_features)\n",
        "#     _, indices = knn.kneighbors(avg_features_scaled)\n",
        "\n",
        "#     # Convert URIs to playable URLs\n",
        "#     recommended = []\n",
        "#     for uri in df.iloc[indices[0]]['uri']:\n",
        "#         if uri.startswith('spotify:track:'):\n",
        "#             recommended.append(f\"https://open.spotify.com/track/{uri.split(':')[-1]}\")\n",
        "#         else:\n",
        "#             recommended.append(uri)\n",
        "#     return recommended\n",
        "\n",
        "# @app.route('/recommend/<emotion>')\n",
        "# def recommend(emotion):\n",
        "#     recommended_songs = get_recommendations(emotion)\n",
        "\n",
        "#     if isinstance(recommended_songs, dict) and 'error' in recommended_songs:\n",
        "#         return jsonify(recommended_songs), 400\n",
        "\n",
        "#     return jsonify({\n",
        "#         'predicted_emotion': emotion,\n",
        "#         'recommended_songs': recommended_songs,\n",
        "#         'dataset_info': {\n",
        "#             'total_songs': len(df),\n",
        "#             f\"songs_for_{emotion}\": len(df[df['labels'].str.lower() == str(emotion).lower()])\n",
        "#         }\n",
        "#     })\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     if scaler and knn and df is not None:\n",
        "#         print(\"Model loaded successfully. Starting server...\")\n",
        "#         app.run()\n",
        "#     else:\n",
        "#         print(\"Failed to load models. Please train the model first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "s6_43RRzpYIS",
        "outputId": "2255645a-4e3f-482b-dc3b-3ceb1240e602"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom flask import Flask, request, jsonify\\nfrom flask_cors import CORS\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport cv2\\nimport numpy as np\\nimport io\\nimport spotipy\\nfrom spotipy.oauth2 import SpotifyClientCredentials\\nimport random\\nfrom pyngrok import ngrok\\nimport joblib\\nimport pandas as pd\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Initialize Flask app\\napp = Flask(__name__)\\nCORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\\n\\n# Spotify API credentials\\nSPOTIFY_CLIENT_ID = \\'6d07d59d7bd6434e93d9522ed98932a5\\'\\nSPOTIFY_CLIENT_SECRET = \\'9f948e31b5304c27b6fb5661b95c45b3\\'\\nsp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\\n    client_id=SPOTIFY_CLIENT_ID,\\n    client_secret=SPOTIFY_CLIENT_SECRET\\n))\\n\\n# Set ngrok auth token\\nngrok.set_auth_token(\"2rcQPgulcm6DfbhT777NQESmLEC_aewARhgxDRvRDnLSpV1Z\")\\n\\n# ====================== Emotion CNN Setup ======================\\nclass EmotionCNN(nn.Module):\\n    def __init__(self, num_classes):\\n        super(EmotionCNN, self).__init__()\\n        self.conv1 = nn.Sequential(\\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\\n            nn.BatchNorm2d(32),\\n            nn.ReLU(),\\n            nn.MaxPool2d(2)\\n        )\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\\n            nn.BatchNorm2d(64),\\n            nn.ReLU(),\\n            nn.MaxPool2d(2)\\n        )\\n        self.conv3 = nn.Sequential(\\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\\n            nn.BatchNorm2d(128),\\n            nn.ReLU(),\\n            nn.MaxPool2d(2)\\n        )\\n        self.fc1 = nn.Linear(128 * 6 * 6, 128)\\n        self.fc2 = nn.Linear(128, num_classes)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.conv2(x)\\n        x = self.conv3(x)\\n        x = x.view(x.size(0), -1)\\n        x = F.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n\\n# Load emotion model\\nmodel = EmotionCNN(num_classes=3)\\nmodel.load_state_dict(torch.load(\\'emotion_cnn.pth\\'))\\nmodel.eval()\\n\\n# Emotion classes\\nclass_indices = {\\'happy\\': 0, \\'neutral\\': 1, \\'surprise\\': 2}\\nindex_to_class = {v: k for k, v in class_indices.items()}\\n\\n# ====================== Music Recommendation Setup ======================\\n# Load KNN recommendation models\\ntry:\\n    scaler = joblib.load(\\'scaler.joblib\\')\\n    knn = joblib.load(\\'knn_model.joblib\\')\\n    df = pd.read_csv(\\'processed_data.csv\\')\\n    features = [\"danceability\", \"energy\", \"valence\", \"tempo\"]\\n    print(\"Music recommendation models loaded successfully!\")\\nexcept Exception as e:\\n    print(f\"Error loading music models: {str(e)}\")\\n    scaler, knn, df = None, None, None\\n\\n# ====================== Helper Functions ======================\\ndef preprocess_image(image_data):\\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \\'haarcascade_frontalface_default.xml\\')\\n    file_bytes = np.frombuffer(image_data, np.uint8)\\n    img = cv2.imdecode(file_bytes, cv2.IMREAD_GRAYSCALE)\\n\\n    # Detect face\\n    faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5)\\n\\n    if len(faces) == 0:\\n        raise Exception(\"No face detected\")\\n\\n    # Crop the first face found\\n    (x, y, w, h) = faces[0]\\n    face = img[y:y+h, x:x+w]\\n\\n    # Resize and normalize\\n    face = cv2.resize(face, (48, 48))\\n    face = face / 255.0\\n    face = np.expand_dims(face, axis=0)\\n    face = np.expand_dims(face, axis=0)\\n    img_tensor = torch.tensor(face, dtype=torch.float32)\\n\\n    return img_tensor\\n\\ndef get_knn_recommendations(emotion):\\n    # Get recommendations using the trained KNN model\\n    if scaler is None or knn is None or df is None:\\n        return {\"error\": \"Music recommendation models not loaded properly\"}\\n\\n    try:\\n        # Convert emotion to string (handles both numeric and string labels)\\n        emotion_str = str(emotion)\\n\\n        # Check if labels are numeric or strings\\n        if pd.api.types.is_numeric_dtype(df[\\'labels\\']):\\n            # If labels are numeric (0,1,2), convert emotion to corresponding number\\n            emotion_label = int(emotion) if isinstance(emotion, (int, float)) else class_indices.get(emotion.lower(), 0)\\n            emotion_songs = df[df[\\'labels\\'] == emotion_label]\\n        else:\\n            # If labels are strings (\"happy\", etc.), match case-insensitive\\n            emotion_songs = df[df[\\'labels\\'].str.lower() == emotion_str.lower()]\\n\\n        if len(emotion_songs) == 0:\\n            return {\"error\": f\"No songs found for emotion: {emotion}\"}\\n\\n        # KNN-based recommendations\\n        avg_features = emotion_songs[features].mean().values.reshape(1, -1)\\n        avg_features_scaled = scaler.transform(avg_features)\\n        _, indices = knn.kneighbors(avg_features_scaled)\\n\\n        # Convert URIs to playable URLs and get track info\\n        recommendations = []\\n        for uri in df.iloc[indices[0]][\\'uri\\']:\\n            if uri.startswith(\\'spotify:track:\\'):\\n                track_id = uri.split(\\':\\')[-1]\\n                try:\\n                    track = sp.track(uri)\\n                    recommendations.append({\\n                        \\'name\\': track[\\'name\\'],\\n                        \\'artist\\': track[\\'artists\\'][0][\\'name\\'],\\n                        \\'link\\': track[\\'external_urls\\'][\\'spotify\\'],\\n                        \\'image\\': track[\\'album\\'][\\'images\\'][0][\\'url\\'] if track[\\'album\\'][\\'images\\'] else None\\n                    })\\n                except:\\n                    recommendations.append({\\n                        \\'link\\': f\"https://open.spotify.com/track/{track_id}\",\\n                        \\'name\\': f\"Track {track_id}\",\\n                        \\'artist\\': \"Unknown\",\\n                        \\'image\\': None\\n                    })\\n            else:\\n                recommendations.append({\\n                    \\'link\\': uri,\\n                    \\'name\\': \"Custom Track\",\\n                    \\'artist\\': \"Unknown\",\\n                    \\'image\\': None\\n                })\\n        return recommendations\\n\\n    except Exception as e:\\n        return {\"error\": f\"Error in recommendation: {str(e)}\"}\\n\\ndef get_spotify_track(emotion):\\n    # Get random track from Spotify based on emotion\\n    results = sp.search(q=f\\'mood:{emotion}\\', type=\\'track\\', limit=10)\\n    tracks = results[\\'tracks\\'][\\'items\\']\\n\\n    if not tracks:\\n        # Default fallback track\\n        track = sp.track(\\'spotify:track:4uLU6hMCjMI75M1A2tKUQC\\')\\n        return {\\n            \\'name\\': track[\\'name\\'],\\n            \\'artist\\': track[\\'artists\\'][0][\\'name\\'],\\n            \\'link\\': track[\\'external_urls\\'][\\'spotify\\'],\\n            \\'image\\': track[\\'album\\'][\\'images\\'][0][\\'url\\'] if track[\\'album\\'][\\'images\\'] else None\\n        }\\n\\n    track = random.choice(tracks[:5])  # Random from top 5 tracks\\n    return {\\n        \\'name\\': track[\\'name\\'],\\n        \\'artist\\': track[\\'artists\\'][0][\\'name\\'],\\n        \\'link\\': track[\\'external_urls\\'][\\'spotify\\'],\\n        \\'image\\': track[\\'album\\'][\\'images\\'][0][\\'url\\'] if track[\\'album\\'][\\'images\\'] else None\\n    }\\n\\n# ====================== API Endpoints ======================\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    if \\'file\\' not in request.files:\\n        return jsonify({\\'error\\': \\'No file uploaded\\'}), 400\\n\\n    file = request.files[\\'file\\']\\n    if file.filename == \\'\\':\\n        return jsonify({\\'error\\': \\'No file selected\\'}), 400\\n\\n    try:\\n        # Read and preprocess the image\\n        image_data = file.read()\\n        processed_img = preprocess_image(image_data)\\n\\n        # Get emotion prediction\\n        prediction = model(processed_img)\\n        predicted_class_index = torch.argmax(prediction, dim=1).item()\\n        predicted_class_label = index_to_class[predicted_class_index]\\n\\n        # Get both KNN and Spotify recommendations\\n        knn_recommendations = get_knn_recommendations(predicted_class_label)\\n        spotify_recommendation = get_spotify_track(predicted_class_label)\\n\\n        response = {\\n            \\'predicted_class\\': predicted_class_label,\\n            \\'knn_recommendations\\': knn_recommendations if not isinstance(knn_recommendations, dict) else [],\\n            \\'spotify_recommendation\\': spotify_recommendation,\\n            \\'model_used\\': \\'knn\\' if not (isinstance(knn_recommendations, dict) and \\'error\\' in knn_recommendations) else \\'spotify\\'\\n        }\\n        return jsonify(response), 200\\n\\n    except Exception as e:\\n        return jsonify({\\'error\\': str(e)}), 500\\n\\n@app.route(\\'/recommend/<emotion>\\', methods=[\\'GET\\'])\\ndef recommend(emotion):\\n    # Direct recommendation endpoint (for testing)\\n    try:\\n        knn_recommendations = get_knn_recommendations(emotion)\\n        spotify_recommendation = get_spotify_track(emotion)\\n\\n        response = {\\n            \\'requested_emotion\\': emotion,\\n            \\'knn_recommendations\\': knn_recommendations if not isinstance(knn_recommendations, dict) else [],\\n            \\'spotify_recommendation\\': spotify_recommendation,\\n            \\'model_used\\': \\'knn\\' if not (isinstance(knn_recommendations, dict) and \\'error\\' in knn_recommendations) else \\'spotify\\'\\n        }\\n        return jsonify(response), 200\\n    except Exception as e:\\n        return jsonify({\\'error\\': str(e)}), 500\\n\\nif __name__ == \\'__main__\\':\\n    # Start ngrok tunnel\\n    ngrok_tunnel = ngrok.connect(5000)\\n    print(\\' * Public URL:\\', ngrok_tunnel.public_url)\\n\\n    # Run Flask app\\n    app.run(port=5000)\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import io\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import random\n",
        "from pyngrok import ngrok\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "\n",
        "# Spotify API credentials\n",
        "SPOTIFY_CLIENT_ID = '6d07d59d7bd6434e93d9522ed98932a5'\n",
        "SPOTIFY_CLIENT_SECRET = '9f948e31b5304c27b6fb5661b95c45b3'\n",
        "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
        "    client_id=SPOTIFY_CLIENT_ID,\n",
        "    client_secret=SPOTIFY_CLIENT_SECRET\n",
        "))\n",
        "\n",
        "# Set ngrok auth token\n",
        "ngrok.set_auth_token(\"2rcQPgulcm6DfbhT777NQESmLEC_aewARhgxDRvRDnLSpV1Z\")\n",
        "\n",
        "# ====================== Emotion CNN Setup ======================\n",
        "class EmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EmotionCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load emotion model\n",
        "model = EmotionCNN(num_classes=3)\n",
        "model.load_state_dict(torch.load('emotion_cnn.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Emotion classes\n",
        "class_indices = {'happy': 0, 'neutral': 1, 'surprise': 2}\n",
        "index_to_class = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# ====================== Music Recommendation Setup ======================\n",
        "# Load KNN recommendation models\n",
        "try:\n",
        "    scaler = joblib.load('scaler.joblib')\n",
        "    knn = joblib.load('knn_model.joblib')\n",
        "    df = pd.read_csv('processed_data.csv')\n",
        "    features = [\"danceability\", \"energy\", \"valence\", \"tempo\"]\n",
        "    print(\"Music recommendation models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading music models: {str(e)}\")\n",
        "    scaler, knn, df = None, None, None\n",
        "\n",
        "# ====================== Helper Functions ======================\n",
        "def preprocess_image(image_data):\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    file_bytes = np.frombuffer(image_data, np.uint8)\n",
        "    img = cv2.imdecode(file_bytes, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Detect face\n",
        "    faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        raise Exception(\"No face detected\")\n",
        "\n",
        "    # Crop the first face found\n",
        "    (x, y, w, h) = faces[0]\n",
        "    face = img[y:y+h, x:x+w]\n",
        "\n",
        "    # Resize and normalize\n",
        "    face = cv2.resize(face, (48, 48))\n",
        "    face = face / 255.0\n",
        "    face = np.expand_dims(face, axis=0)\n",
        "    face = np.expand_dims(face, axis=0)\n",
        "    img_tensor = torch.tensor(face, dtype=torch.float32)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "def get_knn_recommendations(emotion):\n",
        "    # Get recommendations using the trained KNN model\n",
        "    if scaler is None or knn is None or df is None:\n",
        "        return {\"error\": \"Music recommendation models not loaded properly\"}\n",
        "\n",
        "    try:\n",
        "        # Convert emotion to string (handles both numeric and string labels)\n",
        "        emotion_str = str(emotion)\n",
        "\n",
        "        # Check if labels are numeric or strings\n",
        "        if pd.api.types.is_numeric_dtype(df['labels']):\n",
        "            # If labels are numeric (0,1,2), convert emotion to corresponding number\n",
        "            emotion_label = int(emotion) if isinstance(emotion, (int, float)) else class_indices.get(emotion.lower(), 0)\n",
        "            emotion_songs = df[df['labels'] == emotion_label]\n",
        "        else:\n",
        "            # If labels are strings (\"happy\", etc.), match case-insensitive\n",
        "            emotion_songs = df[df['labels'].str.lower() == emotion_str.lower()]\n",
        "\n",
        "        if len(emotion_songs) == 0:\n",
        "            return {\"error\": f\"No songs found for emotion: {emotion}\"}\n",
        "\n",
        "        # KNN-based recommendations\n",
        "        avg_features = emotion_songs[features].mean().values.reshape(1, -1)\n",
        "        avg_features_scaled = scaler.transform(avg_features)\n",
        "        _, indices = knn.kneighbors(avg_features_scaled)\n",
        "\n",
        "        # Convert URIs to playable URLs and get track info\n",
        "        recommendations = []\n",
        "        for uri in df.iloc[indices[0]]['uri']:\n",
        "            if uri.startswith('spotify:track:'):\n",
        "                track_id = uri.split(':')[-1]\n",
        "                try:\n",
        "                    track = sp.track(uri)\n",
        "                    recommendations.append({\n",
        "                        'name': track['name'],\n",
        "                        'artist': track['artists'][0]['name'],\n",
        "                        'link': track['external_urls']['spotify'],\n",
        "                        'image': track['album']['images'][0]['url'] if track['album']['images'] else None\n",
        "                    })\n",
        "                except:\n",
        "                    recommendations.append({\n",
        "                        'link': f\"https://open.spotify.com/track/{track_id}\",\n",
        "                        'name': f\"Track {track_id}\",\n",
        "                        'artist': \"Unknown\",\n",
        "                        'image': None\n",
        "                    })\n",
        "            else:\n",
        "                recommendations.append({\n",
        "                    'link': uri,\n",
        "                    'name': \"Custom Track\",\n",
        "                    'artist': \"Unknown\",\n",
        "                    'image': None\n",
        "                })\n",
        "        return recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error in recommendation: {str(e)}\"}\n",
        "\n",
        "def get_spotify_track(emotion):\n",
        "    # Get random track from Spotify based on emotion\n",
        "    results = sp.search(q=f'mood:{emotion}', type='track', limit=10)\n",
        "    tracks = results['tracks']['items']\n",
        "\n",
        "    if not tracks:\n",
        "        # Default fallback track\n",
        "        track = sp.track('spotify:track:4uLU6hMCjMI75M1A2tKUQC')\n",
        "        return {\n",
        "            'name': track['name'],\n",
        "            'artist': track['artists'][0]['name'],\n",
        "            'link': track['external_urls']['spotify'],\n",
        "            'image': track['album']['images'][0]['url'] if track['album']['images'] else None\n",
        "        }\n",
        "\n",
        "    track = random.choice(tracks[:5])  # Random from top 5 tracks\n",
        "    return {\n",
        "        'name': track['name'],\n",
        "        'artist': track['artists'][0]['name'],\n",
        "        'link': track['external_urls']['spotify'],\n",
        "        'image': track['album']['images'][0]['url'] if track['album']['images'] else None\n",
        "    }\n",
        "\n",
        "# ====================== API Endpoints ======================\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file uploaded'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No file selected'}), 400\n",
        "\n",
        "    try:\n",
        "        # Read and preprocess the image\n",
        "        image_data = file.read()\n",
        "        processed_img = preprocess_image(image_data)\n",
        "\n",
        "        # Get emotion prediction\n",
        "        prediction = model(processed_img)\n",
        "        predicted_class_index = torch.argmax(prediction, dim=1).item()\n",
        "        predicted_class_label = index_to_class[predicted_class_index]\n",
        "\n",
        "        # Get both KNN and Spotify recommendations\n",
        "        knn_recommendations = get_knn_recommendations(predicted_class_label)\n",
        "        spotify_recommendation = get_spotify_track(predicted_class_label)\n",
        "\n",
        "        response = {\n",
        "            'predicted_class': predicted_class_label,\n",
        "            'knn_recommendations': knn_recommendations if not isinstance(knn_recommendations, dict) else [],\n",
        "            'spotify_recommendation': spotify_recommendation,\n",
        "            'model_used': 'knn' if not (isinstance(knn_recommendations, dict) and 'error' in knn_recommendations) else 'spotify'\n",
        "        }\n",
        "        return jsonify(response), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/recommend/<emotion>', methods=['GET'])\n",
        "def recommend(emotion):\n",
        "    # Direct recommendation endpoint (for testing)\n",
        "    try:\n",
        "        knn_recommendations = get_knn_recommendations(emotion)\n",
        "        spotify_recommendation = get_spotify_track(emotion)\n",
        "\n",
        "        response = {\n",
        "            'requested_emotion': emotion,\n",
        "            'knn_recommendations': knn_recommendations if not isinstance(knn_recommendations, dict) else [],\n",
        "            'spotify_recommendation': spotify_recommendation,\n",
        "            'model_used': 'knn' if not (isinstance(knn_recommendations, dict) and 'error' in knn_recommendations) else 'spotify'\n",
        "        }\n",
        "        return jsonify(response), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Start ngrok tunnel\n",
        "    ngrok_tunnel = ngrok.connect(5000)\n",
        "    print(' * Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "    # Run Flask app\n",
        "    app.run(port=5000)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDeKO3NwEa8y",
        "outputId": "15d8bada-19ab-49da-8fb0-d08b4cd025c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "Collecting flask-cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-5.0.1\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 starlette-0.46.2 uvicorn-0.34.1\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.20\n",
            "Collecting spotipy\n",
            "  Downloading spotipy-2.25.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting redis>=3.5.3 (from spotipy)\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from spotipy) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from spotipy) (2.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->spotipy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->spotipy) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->spotipy) (2025.1.31)\n",
            "Downloading spotipy-2.25.1-py3-none-any.whl (31 kB)\n",
            "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis, spotipy\n",
            "Successfully installed redis-5.2.1 spotipy-2.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "!pip install flask-cors\n",
        "!pip install fastapi uvicorn tensorflow numpy opencv-python pillow\n",
        "!pip install python-multipart\n",
        "!pip install spotipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjqTK0gW-jLI",
        "outputId": "2a450c98-b8f5-483f-c700-d6138f33442c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Music recommendation models loaded successfully!\n",
            " * Public URL: https://9cb5-34-125-205-230.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 01:27:40] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import io\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import random\n",
        "from pyngrok import ngrok\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "\n",
        "# Spotify API credentials\n",
        "SPOTIFY_CLIENT_ID = '6d07d59d7bd6434e93d9522ed98932a5'\n",
        "SPOTIFY_CLIENT_SECRET = '9f948e31b5304c27b6fb5661b95c45b3'\n",
        "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
        "    client_id=SPOTIFY_CLIENT_ID,\n",
        "    client_secret=SPOTIFY_CLIENT_SECRET\n",
        "))\n",
        "\n",
        "# Set ngrok auth token\n",
        "ngrok.set_auth_token(\"2rcQPgulcm6DfbhT777NQESmLEC_aewARhgxDRvRDnLSpV1Z\")\n",
        "\n",
        "# ====================== Emotion CNN Setup ======================\n",
        "class EmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EmotionCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load emotion model\n",
        "model = EmotionCNN(num_classes=3)\n",
        "model.load_state_dict(torch.load('emotion_cnn.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Emotion classes mapping for CNN\n",
        "class_indices = {'happy': 0, 'neutral': 1, 'surprise': 2}\n",
        "index_to_class = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# Emotion mapping for KNN model (happy=1, neutral=3, surprise=2)\n",
        "knn_emotion_mapping = {\n",
        "    'happy': 1,\n",
        "    'neutral': 3,\n",
        "    'surprise': 2\n",
        "}\n",
        "\n",
        "# ====================== Music Recommendation Setup ======================\n",
        "# Load KNN recommendation models\n",
        "try:\n",
        "    scaler = joblib.load('scaler.joblib')\n",
        "    knn = joblib.load('knn_model.joblib')\n",
        "    df = pd.read_csv('processed_data.csv')\n",
        "    features = [\"danceability\", \"energy\", \"valence\", \"tempo\"]\n",
        "    print(\"Music recommendation models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading music models: {str(e)}\")\n",
        "    scaler, knn, df = None, None, None\n",
        "\n",
        "#---------------------Helper Functions-------------------\n",
        "def preprocess_image(image_data):\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    file_bytes = np.frombuffer(image_data, np.uint8)\n",
        "    img = cv2.imdecode(file_bytes, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Detect face\n",
        "    faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        raise Exception(\"No face detected\")\n",
        "\n",
        "    # Crop the first face found\n",
        "    (x, y, w, h) = faces[0]\n",
        "    face = img[y:y+h, x:x+w]\n",
        "\n",
        "    # Resize and normalize\n",
        "    face = cv2.resize(face, (48, 48))\n",
        "    face = face / 255.0\n",
        "    face = np.expand_dims(face, axis=0)\n",
        "    face = np.expand_dims(face, axis=0)\n",
        "    img_tensor = torch.tensor(face, dtype=torch.float32)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "def get_knn_recommendations(emotion):\n",
        "    \"\"\"Get recommendations using the trained KNN model\"\"\"\n",
        "    if scaler is None or knn is None or df is None:\n",
        "        return {\"error\": \"Music recommendation models not loaded properly\"}\n",
        "\n",
        "    try:\n",
        "        # Convert emotion to the KNN label (happy=1, neutral=3, surprise=2)\n",
        "        if isinstance(emotion, str):\n",
        "            knn_label = knn_emotion_mapping.get(emotion.lower(), 1)  # Default to happy if not found\n",
        "        else:\n",
        "            # If numeric, map from CNN index to KNN label\n",
        "            if emotion == 0:  # CNN happy\n",
        "                knn_label = 1\n",
        "            elif emotion == 1:  # CNN neutral\n",
        "                knn_label = 3\n",
        "            elif emotion == 2:  # CNN surprise\n",
        "                knn_label = 2\n",
        "            else:\n",
        "                knn_label = 1  # Default to happy\n",
        "\n",
        "        # Get songs with the matching KNN label\n",
        "        emotion_songs = df[df['labels'] == knn_label]\n",
        "\n",
        "        if len(emotion_songs) == 0:\n",
        "            return {\"error\": f\"No songs found for emotion label: {knn_label}\"}\n",
        "\n",
        "        # KNN-based recommendations\n",
        "        avg_features = emotion_songs[features].mean().values.reshape(1, -1)\n",
        "        avg_features_scaled = scaler.transform(avg_features)\n",
        "        _, indices = knn.kneighbors(avg_features_scaled)\n",
        "\n",
        "        # Convert URIs to playable URLs and get track info\n",
        "        recommendations = []\n",
        "        for uri in df.iloc[indices[0]]['uri']:\n",
        "            if uri.startswith('spotify:track:'):\n",
        "                track_id = uri.split(':')[-1]\n",
        "                try:\n",
        "                    track = sp.track(uri)\n",
        "                    recommendations.append({\n",
        "                        'name': track['name'],\n",
        "                        'artist': track['artists'][0]['name'],\n",
        "                        'link': track['external_urls']['spotify'],\n",
        "                        'image': track['album']['images'][0]['url'] if track['album']['images'] else None\n",
        "                    })\n",
        "                except:\n",
        "                    recommendations.append({\n",
        "                        'link': f\"https://open.spotify.com/track/{track_id}\",\n",
        "                        'name': f\"Track {track_id}\",\n",
        "                        'artist': \"Unknown\",\n",
        "                        'image': None\n",
        "                    })\n",
        "            else:\n",
        "                recommendations.append({\n",
        "                    'link': uri,\n",
        "                    'name': \"Custom Track\",\n",
        "                    'artist': \"Unknown\",\n",
        "                    'image': None\n",
        "                })\n",
        "        return recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error in recommendation: {str(e)}\"}\n",
        "\n",
        "def get_spotify_track(emotion):\n",
        "    \"\"\"Get random track from Spotify based on emotion\"\"\"\n",
        "    emotion_str = index_to_class.get(emotion, emotion) if isinstance(emotion, int) else emotion\n",
        "    results = sp.search(q=f'mood:{emotion_str}', type='track', limit=10)\n",
        "    tracks = results['tracks']['items']\n",
        "\n",
        "    if not tracks:\n",
        "        # Default fallback track\n",
        "        track = sp.track('spotify:track:4uLU6hMCjMI75M1A2tKUQC')\n",
        "        return {\n",
        "            'name': track['name'],\n",
        "            'artist': track['artists'][0]['name'],\n",
        "            'link': track['external_urls']['spotify'],\n",
        "            'image': track['album']['images'][0]['url'] if track['album']['images'] else None\n",
        "        }\n",
        "\n",
        "    track = random.choice(tracks[:5])  # Random from top 5 tracks\n",
        "    return {\n",
        "        'name': track['name'],\n",
        "        'artist': track['artists'][0]['name'],\n",
        "        'link': track['external_urls']['spotify'],\n",
        "        'image': track['album']['images'][0]['url'] if track['album']['images'] else None\n",
        "    }\n",
        "\n",
        "#----------------API Endpoints--------------------\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file uploaded'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No file selected'}), 400\n",
        "\n",
        "    try:\n",
        "        # Read and preprocess the image\n",
        "        image_data = file.read()\n",
        "        processed_img = preprocess_image(image_data)\n",
        "\n",
        "        # Get emotion prediction\n",
        "        prediction = model(processed_img)\n",
        "        predicted_class_index = torch.argmax(prediction, dim=1).item()\n",
        "        predicted_class_label = index_to_class[predicted_class_index]\n",
        "\n",
        "        # Get both KNN and Spotify recommendations\n",
        "        knn_recommendations = get_knn_recommendations(predicted_class_index)  # Pass the index to handle mapping\n",
        "        spotify_recommendation = get_spotify_track(predicted_class_index)\n",
        "\n",
        "        response = {\n",
        "            'predicted_class': predicted_class_label,\n",
        "            'knn_recommendations': knn_recommendations if not isinstance(knn_recommendations, dict) else [],\n",
        "            'spotify_recommendation': spotify_recommendation,\n",
        "            'model_used': 'knn' if not (isinstance(knn_recommendations, dict) and 'error' in knn_recommendations) else 'spotify'\n",
        "        }\n",
        "        return jsonify(response), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/recommend/<emotion>', methods=['GET'])\n",
        "def recommend(emotion):\n",
        "    \"\"\"Direct recommendation endpoint (for testing)\"\"\"\n",
        "    try:\n",
        "        # Handle both string and numeric emotion inputs\n",
        "        if emotion.isdigit():\n",
        "            emotion_int = int(emotion)\n",
        "            if emotion_int == 0:\n",
        "                emotion_str = 'happy'\n",
        "            elif emotion_int == 1:\n",
        "                emotion_str = 'neutral'\n",
        "            elif emotion_int == 2:\n",
        "                emotion_str = 'surprise'\n",
        "            else:\n",
        "                emotion_str = 'happy'\n",
        "        else:\n",
        "            emotion_str = emotion.lower()\n",
        "            emotion_int = class_indices.get(emotion_str, 0)\n",
        "\n",
        "        knn_recommendations = get_knn_recommendations(emotion_int)  # Pass the index to handle mapping\n",
        "        spotify_recommendation = get_spotify_track(emotion_str)\n",
        "\n",
        "        response = {\n",
        "            'requested_emotion': emotion_str,\n",
        "            'knn_recommendations': knn_recommendations if not isinstance(knn_recommendations, dict) else [],\n",
        "            'spotify_recommendation': spotify_recommendation,\n",
        "            'model_used': 'knn' if not (isinstance(knn_recommendations, dict) and 'error' in knn_recommendations) else 'spotify'\n",
        "        }\n",
        "        return jsonify(response), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Start ngrok tunnel\n",
        "    ngrok_tunnel = ngrok.connect(5000)\n",
        "    print(' * Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "    # Run Flask app\n",
        "    app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}